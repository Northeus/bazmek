"""
LLM hooks for Ollama API.

See classes `LLM` and `Config` for LLM set-up. Function `prompt` then send API
request to the LLM using either provided or default config.
"""

from __future__ import annotations

from bazmek import utils
from transformers import AutoTokenizer  # pyright: ignore
from typing import (overload, Literal, Mapping, NamedTuple, ReadOnly,
                    Sequence, TypedDict)

import aiohttp
import huggingface_hub
import json
import os


class LLM(NamedTuple):
    """
    LLM connection information.

    The `url` shold not end with '/'. A `model` parameter is the exact model
    name from Ollama.
    """

    class Config(TypedDict, total=False):
        """
        LLM parameters.

        You might want to increase the parameter `num_ctx` that represents the
        context window as it default value for model is usually small.

        For more information see:
        https://github.com/ollama/ollama/blob/main/docs/modelfile.md
        """

        mirostat: ReadOnly[int]
        mirostat_eta: ReadOnly[float]
        mirostat_tau: ReadOnly[float]
        num_ctx: ReadOnly[int]
        repeat_last_n: ReadOnly[int]
        repeat_penalty: ReadOnly[float]
        temperature: ReadOnly[float]
        seed: ReadOnly[int]
        stop: ReadOnly[list[str]]
        num_predict: ReadOnly[int]
        top_k: ReadOnly[int]
        top_p: ReadOnly[float]
        min_p: ReadOnly[float]

    Model = Literal['llama3.3', 'gemma3', 'phi4']

    url: str
    model: Model
    timeout_ms: int | None = None
    config: Config = Config()


JSON = str | int | float | bool | Sequence['JSON'] | Mapping[str, 'JSON']


class Message(NamedTuple):
    """
    LLM chat message with the information about the author.

    Use class methods `system`, `assistant`, and `user` for neater usage.
    Proveded `images` should be encoded in base64.
    """

    role: Literal['system', 'assistant', 'user']
    content: str
    images: tuple[str, ...] | None = None
    tool_calls: JSON | None = None

    @classmethod
    def system(cls, message: str) -> Message:
        """Create system message."""
        return Message(role='system', content=message)

    @classmethod
    def user(cls,
             message: str,
             *,
             images: tuple[str, ...] | None = None
             ) -> Message:
        """Create user message."""
        return Message(role='user', content=message, images=images)


@overload
async def prompt(llm: LLM,
                 messages: list[Message],
                 *,
                 tools: JSON | None = None
                 ) -> utils.Result[Message]:
    ...


@overload
async def prompt(llm: LLM,
                 messages: list[Message],
                 *,
                 schema: JSON,
                 tools: JSON | None = None
                 ) -> utils.Result[JSON]:
    ...


async def prompt(llm: LLM,
                 messages: list[Message],
                 *,
                 schema: JSON | None = None,
                 tools: JSON | None = None
                 ) -> utils.Result[Message | JSON]:
    """
    Prompt LLM with messages to obtain a response.

    If a `scheme` is provided, then the result is an dictionary representation
    with the same form given by the scheme. Using `scheme` can hinder the LLM
    accuracy. Make sure, that answer does not preceed reasoning to mittigate
    halucinations generated by LLM. The format of scheme is:
    ```
    {
        "type": "object" | "array" | "bool" | "integer" | "float" | "string",
        "description": "...",

        // For arrays:
        "items": { /* Format of another objects */ }

        // For objects:
        "properties": {
            "property_name": { /* Format of another objects */ },
        }
        "required": ["property_name"]

        // For enums (ommit "type"):
        "enum": [ /* Possible values */ ]
    }
    ```

    Functions for LLM can be provided using `tools` parameter. This parameter
    should contain JSON array of objects with a name, description, and
    parameters scheme. Example:
    ```
    tools = [
        {
            "name": "get_current_temperature",
            "description": "...",
            "parameters": /* Scheme like object */
        }
    ]
    response = {
        "role": "assistant",
        "content": "",
        "tool_calls": [
            {
                "function": {
                    "name": "get_current_temperature",
                    "arguments": /* Data in the structure based on scheme */
                }
            }
        ]
    }
    next_message = {
        "role": "tool",
        "content": "42"
    }
    ```

    Image can be provided as a base64 encoded string. Ensure, that the size of
    the image match the size required by the model you are using.

    Additionally, LLM parameters can be configured using `config`.
    """
    data = {
        'model': llm.model,
        'messages': [x._asdict() for x in messages],
        'format': schema,
        'tools': tools,
        'options': llm.config,
        'stream': False
    }

    timeout_seconds = llm.timeout_ms and llm.timeout_ms / 1000
    async with (
            aiohttp.ClientSession(timeout=timeout_seconds) as session,
            session.post(llm.url + '/api/chat', json=data) as response):
        if not response.ok:
            return utils.Error(message=f'Invalid response: {str(response)}')

        body = await response.json()

    match schema:
        case None:
            return Message(**body['message'])
        case _:
            return json.loads(body['message']['content'])


def tokenize(huggingface_model: str, messages: list[Message]) -> list[int]:
    """
    Tokineze messages using tokenizer from HuggingFace.

    This function requires environment variable `HF_TOKEN` to consains the
    HuggingFace API token.
    """
    if huggingface_hub.get_token() is None:
        huggingface_hub.login(os.getenv('HF_TOKEN'))

    tokenizer = AutoTokenizer.from_pretrained(huggingface_model)
    return tokenizer.apply_chat_template([x._asdict() for x in messages])
